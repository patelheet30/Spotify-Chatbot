{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 1**\n",
    "\n",
    "- Importing Libraries\n",
    "- Setting Global Variables\n",
    "- Setting Seeds for random data splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import EfficientNetB0, MobileNetV2, ResNet50\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(\n",
    "            gpus[0],\n",
    "            \"GPU\",\n",
    "        )\n",
    "\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "        logical_gpus = tf.config.list_logical_devices(\"GPU\")\n",
    "        print(f\"Using {len(logical_gpus)} GPU(s): {logical_gpus}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU configuration error: {e}\")\n",
    "else:\n",
    "    print(\"No GPU found. Using CPU.\")\n",
    "\n",
    "\n",
    "policy = tf.keras.mixed_precision.Policy(\"mixed_float16\")\n",
    "tf.keras.mixed_precision.set_global_policy(policy)\n",
    "print(f\"Compute dtype: {policy.compute_dtype}\")\n",
    "print(f\"Variable dtype: {policy.variable_dtype}\")\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "\n",
    "ERA_RANGES = [\n",
    "    (1980, 2000, \"Container Era (CDs, Cassettes)\"),\n",
    "    (2000, 2024, \"Digital Era (Streaming, Downloads)\"),\n",
    "]\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "DATA_DIR = Path(\"album_covers_data\")\n",
    "TRAIN_DIR = DATA_DIR / \"train\"\n",
    "TEST_DIR = DATA_DIR / \"test\"\n",
    "OUTPUT_DIR = Path(\"output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 2**\n",
    "\n",
    "- Get Era Labels\n",
    "- Get Era Indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_era_label(year):\n",
    "    for start_year, end_year, era in ERA_RANGES:\n",
    "        if start_year <= year <= end_year:\n",
    "            return era\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_era_index(year):\n",
    "    for i, (start_year, end_year, _) in enumerate(ERA_RANGES):\n",
    "        if start_year <= year <= end_year:\n",
    "            return i\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 3**\n",
    "\n",
    "- Count images in year-based subdirectories\n",
    "- Outputs training and dataset counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images_per_year(directory):\n",
    "    year_counts = {}\n",
    "\n",
    "    for year_folder in sorted(os.listdir(directory)):\n",
    "        if not year_folder.isdigit():\n",
    "            continue\n",
    "\n",
    "        year = int(year_folder)\n",
    "        year_path = directory / year_folder\n",
    "        if os.path.isdir(year_path):\n",
    "            num_images = len(\n",
    "                [\n",
    "                    f\n",
    "                    for f in os.listdir(year_path)\n",
    "                    if f.endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "                ]\n",
    "            )\n",
    "            year_counts[year] = num_images\n",
    "\n",
    "    return year_counts\n",
    "\n",
    "\n",
    "print(\"Counting images per year in training data...\")\n",
    "train_counts = count_images_per_year(TRAIN_DIR)\n",
    "test_counts = count_images_per_year(TEST_DIR)\n",
    "\n",
    "print(\"Training data counts:\")\n",
    "for year, count in sorted(train_counts.items()):\n",
    "    print(f\"{year}: {count}\")\n",
    "\n",
    "print(\"\\nTest data counts:\")\n",
    "for year, count in sorted(test_counts.items()):\n",
    "    print(f\"{year}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 4**\n",
    "\n",
    "- Processes year-based subdirectories\n",
    "- Displays counts in text and barplot formats for training and testing datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(list(train_counts.items()), columns=[\"Year\", \"TrainCount\"])\n",
    "test_df = pd.DataFrame(list(test_counts.items()), columns=[\"Year\", \"TestCount\"])\n",
    "\n",
    "counts_df = pd.merge(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    on=\"Year\",\n",
    "    how=\"outer\",\n",
    ").fillna(0)\n",
    "counts_df[\"TotalCount\"] = counts_df[\"TrainCount\"] + counts_df[\"TestCount\"]\n",
    "\n",
    "counts_df[\"TrainPct\"] = counts_df[\"TrainCount\"] / counts_df[\"TotalCount\"] * 100\n",
    "counts_df[\"TestPct\"] = counts_df[\"TestCount\"] / counts_df[\"TotalCount\"] * 100\n",
    "\n",
    "counts_df[\"Era\"] = counts_df[\"Year\"].apply(get_era_label)\n",
    "\n",
    "print(\"\\nData Distribution by Era:\")\n",
    "print(counts_df[[\"Year\", \"TrainCount\", \"TestCount\", \"TotalCount\", \"Era\"]])\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.barplot(\n",
    "    x=\"Year\",\n",
    "    y=\"TotalCount\",\n",
    "    hue=\"Era\",\n",
    "    data=counts_df,\n",
    "    palette=\"viridis\",\n",
    ")\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Number of Album Covers by Year\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"album_covers_by_year.png\")\n",
    "plt.show()\n",
    "\n",
    "era_counts = (\n",
    "    counts_df.groupby(\"Era\")\n",
    "    .agg(\n",
    "        {\n",
    "            \"TrainCount\": \"sum\",\n",
    "            \"TestCount\": \"sum\",\n",
    "            \"TotalCount\": \"sum\",\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.barplot(\n",
    "    x=\"Era\",\n",
    "    y=\"TotalCount\",\n",
    "    data=era_counts,\n",
    "    palette=\"magma\",\n",
    ")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Number of Album Covers by Era\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"album_covers_by_era.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 5**\n",
    "\n",
    "- Creates datasets with file paths, years and era labels\n",
    "- Outputs training and testing dataset sizes\n",
    "- Displays class distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(directory, era_mapping=None):\n",
    "    labels = []\n",
    "    years = []\n",
    "    file_paths = []\n",
    "\n",
    "    for year_folder in os.listdir(directory):\n",
    "        if not year_folder.isdigit():\n",
    "            continue\n",
    "        year = int(year_folder)\n",
    "        era_idx = get_era_index(year)\n",
    "\n",
    "        if era_idx is None:\n",
    "            continue\n",
    "\n",
    "        year_path = directory / year_folder\n",
    "\n",
    "        if os.path.isdir(year_path):\n",
    "            for img_file in os.listdir(year_path):\n",
    "                if img_file.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    img_path = year_path / img_file\n",
    "                    labels.append(era_idx)\n",
    "                    years.append(year)\n",
    "                    file_paths.append(str(img_path))\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"filepath\": file_paths,\n",
    "            \"year\": years,\n",
    "            \"era_index\": labels,\n",
    "            \"era\": [ERA_RANGES[idx][2] for idx in labels],\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"\\nCreating training dataset...\")\n",
    "train_df = create_dataset(TRAIN_DIR)\n",
    "test_df = create_dataset(TEST_DIR)\n",
    "\n",
    "print(f\"\\nTraining dataset size: {len(train_df)}\")\n",
    "print(f\"Test dataset size: {len(test_df)}\")\n",
    "\n",
    "print(\"\\nClass distribution in training dataset:\")\n",
    "print(train_df[\"era\"].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nClass distribution in test dataset:\")\n",
    "print(test_df[\"era\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 6**\n",
    "\n",
    "- Combines training and testing dataset into one\n",
    "- Computes balanced class weights for eras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "print(\"\\nCombined dataset size:\", len(combined_df))\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(combined_df[\"era_index\"]),\n",
    "    y=combined_df[\"era_index\"],\n",
    ")\n",
    "\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "print(\"\\nClass weights:\")\n",
    "for era_idx, weight in class_weight_dict.items():\n",
    "    print(f\"Era {ERA_RANGES[era_idx][2]}: {weight:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 7**\n",
    "\n",
    "- Splits combined dataset into training, validation and test sets\n",
    "- Ensures stratification by era index for balanced splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df, final_test_df = train_test_split(\n",
    "    combined_df, test_size=0.15, stratify=combined_df[\"era_index\"], random_state=SEED\n",
    ")\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df,\n",
    "    test_size=0.1765,\n",
    "    stratify=train_val_df[\"era_index\"],\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "print(\"\\nFinal dataset sizes:\")\n",
    "print(f\"Training size: {len(train_df)} ({len(train_df) / len(combined_df) * 100:.2f}%)\")\n",
    "print(f\"Validation size: {len(val_df)} ({len(val_df) / len(combined_df) * 100:.2f}%)\")\n",
    "print(\n",
    "    f\"Test size: {len(final_test_df)} ({len(final_test_df) / len(combined_df) * 100:.2f}%)\"\n",
    ")\n",
    "\n",
    "print(\"\\nClass distribution in splits:\")\n",
    "for name, df in [\n",
    "    (\"Training\", train_df),\n",
    "    (\"Validation\", val_df),\n",
    "    (\"Test\", final_test_df),\n",
    "]:\n",
    "    era_counts = df[\"era\"].value_counts().sort_index()\n",
    "    print(f\"\\n{name} dataset:\")\n",
    "    for era, count in era_counts.items():\n",
    "        pct = count / len(df) * 100\n",
    "        print(f\"{era}: {count} ({pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 8**\n",
    "\n",
    "- Prepares data generatoes for training, validation, and testing\n",
    "- Applies data augmentation (to prevent overfitting and the model from learning images rather than learning patterns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    brightness_range=[0.7, 1.3],\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    channel_shift_range=0.2,\n",
    "    fill_mode=\"nearest\",\n",
    ")\n",
    "\n",
    "valid_test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "train_df[\"era_index\"] = train_df[\"era_index\"].astype(str)\n",
    "val_df[\"era_index\"] = val_df[\"era_index\"].astype(str)\n",
    "final_test_df[\"era_index\"] = final_test_df[\"era_index\"].astype(str)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col=\"filepath\",\n",
    "    y_col=\"era_index\",\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"sparse\",\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_generator = valid_test_datagen.flow_from_dataframe(\n",
    "    dataframe=val_df,\n",
    "    x_col=\"filepath\",\n",
    "    y_col=\"era_index\",\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"sparse\",\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "test_generator = valid_test_datagen.flow_from_dataframe(\n",
    "    dataframe=final_test_df,\n",
    "    x_col=\"filepath\",\n",
    "    y_col=\"era_index\",\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"sparse\",\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "print(\"\\nData Generators Created:\")\n",
    "print(f\"Training generator size: {len(train_generator)}\")\n",
    "print(f\"Validation generator size: {len(val_generator)}\")\n",
    "print(f\"Test generator size: {len(test_generator)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 9**\n",
    "\n",
    "- Saves training, validation, and test datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(OUTPUT_DIR / \"train_data.csv\", index=False)\n",
    "val_df.to_csv(OUTPUT_DIR / \"validation_data.csv\", index=False)\n",
    "final_test_df.to_csv(OUTPUT_DIR / \"test_data.csv\", index=False)\n",
    "\n",
    "print(\"\\nData preparation complete!\")\n",
    "print(\"The next step is to define and train the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_resnet_model(input_shape=(224, 224, 3), num_classes=NUM_CLASSES):\n",
    "    base_model = ResNet50(\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "    )\n",
    "\n",
    "    for layer in base_model.layers[:-30]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model = models.Sequential(\n",
    "        [\n",
    "            base_model,\n",
    "            layers.GlobalAveragePooling2D(),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dense(512, activation=\"relu\"),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(256, activation=\"relu\"),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(num_classes, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_efficientnet_model(input_shape=(224, 224, 3), num_classes=len(ERA_RANGES)):\n",
    "    base_model = EfficientNetB0(\n",
    "        input_shape=input_shape, include_top=False, weights=\"imagenet\"\n",
    "    )\n",
    "\n",
    "    for layer in base_model.layers[:-20]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model = models.Sequential(\n",
    "        [\n",
    "            base_model,\n",
    "            layers.GlobalAveragePooling2D(),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dense(512, activation=\"relu\"),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(256, activation=\"relu\"),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(num_classes, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_learning_rate(epoch):\n",
    "    initial_lr = 5e-5\n",
    "    max_lr = 2e-4\n",
    "    min_lr = 1e-6\n",
    "    warmup_epochs = 2\n",
    "\n",
    "    if epoch < warmup_epochs:\n",
    "        return initial_lr + (max_lr - initial_lr) * (epoch / warmup_epochs)\n",
    "\n",
    "    return float(max_lr * tf.math.exp(0.1 * (warmup_epochs - epoch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_accuracy\",\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1,\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath=OUTPUT_DIR / \"best_model_{epoch:02d}_{val_accuracy:.4f}.keras\",\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        mode=\"max\",\n",
    "        verbose=1,\n",
    "    ),\n",
    "    tf.keras.callbacks.LearningRateScheduler(custom_learning_rate, verbose=1),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUMemoryCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if gpus:\n",
    "            import subprocess\n",
    "\n",
    "            result = subprocess.check_output(\n",
    "                [\n",
    "                    \"nvidia-smi\",\n",
    "                    \"--query-gpu=memory.used,memory.total\",\n",
    "                    \"--format=csv,nounits,noheader\",\n",
    "                ]\n",
    "            )\n",
    "            line = result.decode(\"utf-8\").strip()\n",
    "            memory_used, memory_total = [s.strip() for s in line.split(\",\")]\n",
    "            print(\n",
    "                f\"\\nGPU Memory: {int(memory_used)} MB / {int(memory_total)} MB ({int(memory_used) / int(memory_total) * 100:.1f}%)\"\n",
    "            )\n",
    "\n",
    "\n",
    "callbacks.append(GPUMemoryCallback())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model, train_generator, validation_generator, class_weight_dict, epochs=30\n",
    "):\n",
    "    print(\"Stage 1: Training only top layers...\")\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    history1 = model.fit(\n",
    "        train_generator,\n",
    "        epochs=5,\n",
    "        validation_data=validation_generator,\n",
    "        class_weight=class_weight_dict,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    print(\"Stage 2: Fine-tuning with more layers...\")\n",
    "\n",
    "    if isinstance(model.layers[0], tf.keras.Model) and \"resnet\" in model.layers[0].name:\n",
    "        for layer in model.layers[0].layers[-50:]:\n",
    "            layer.trainable = True\n",
    "\n",
    "    elif (\n",
    "        isinstance(model.layers[0], tf.keras.Model)\n",
    "        and \"efficientnet\" in model.layers[0].name\n",
    "    ):\n",
    "        for layer in model.layers[0].layers[-30:]:\n",
    "            layer.trainable = True\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    history2 = model.fit(\n",
    "        train_generator,\n",
    "        epochs=epochs - 5,\n",
    "        validation_data=validation_generator,\n",
    "        class_weight=class_weight_dict,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    combined_history = {}\n",
    "    for k in history1.history.keys():\n",
    "        combined_history[k] = history1.history[k] + history2.history[k]\n",
    "\n",
    "    return model, combined_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining ResNet model with advanced techniques...\")\n",
    "resnet_model = create_resnet_model()\n",
    "resnet_model, resnet_history = train_model(\n",
    "    resnet_model, train_generator, val_generator, class_weight_dict\n",
    ")\n",
    "\n",
    "resnet_model.save(OUTPUT_DIR / \"resnet_final_model.keras\")\n",
    "print(\"ResNet model training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining EfficientNet model with advanced techniques...\")\n",
    "efficientnet_model = create_efficientnet_model()\n",
    "efficientnet_model, efficientnet_history = train_model(\n",
    "    efficientnet_model, train_generator, val_generator, class_weight_dict\n",
    ")\n",
    "\n",
    "efficientnet_model.save(OUTPUT_DIR / \"efficientnet_final_model.keras\")\n",
    "print(\"EfficientNet model training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(resnet_history[\"accuracy\"], label=\"ResNet Training\")\n",
    "plt.plot(efficientnet_history[\"accuracy\"], label=\"EfficientNet Training\")\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(resnet_history[\"val_accuracy\"], label=\"ResNet Validation\")\n",
    "plt.plot(efficientnet_history[\"val_accuracy\"], label=\"EfficientNet Validation\")\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(resnet_history[\"loss\"], label=\"ResNet Training\")\n",
    "plt.plot(efficientnet_history[\"loss\"], label=\"EfficientNet Training\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(resnet_history[\"val_loss\"], label=\"ResNet Validation\")\n",
    "plt.plot(efficientnet_history[\"val_loss\"], label=\"EfficientNet Validation\")\n",
    "plt.title(\"Validation Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"training_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluating ResNet model on test data...\")\n",
    "resnet_results = resnet_model.evaluate(test_generator, verbose=1)\n",
    "print(f\"ResNet Test Loss: {resnet_results[0]:.4f}\")\n",
    "print(f\"ResNet Test Accuracy: {resnet_results[1]:.4f}\")\n",
    "\n",
    "print(\"\\nEvaluating EfficientNet model on test data...\")\n",
    "efficientnet_results = efficientnet_model.evaluate(test_generator, verbose=1)\n",
    "print(f\"EfficientNet Test Loss: {efficientnet_results[0]:.4f}\")\n",
    "print(f\"EfficientNet Test Accuracy: {efficientnet_results[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(models, image_path, img_size=224):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)  # type: ignore\n",
    "    img = tf.image.resize(img, (img_size, img_size))\n",
    "    img = img / 255.0  # type: ignore\n",
    "    img = tf.expand_dims(img, 0)\n",
    "\n",
    "    predictions = []\n",
    "    for model in models:\n",
    "        pred = model.predict(img)\n",
    "        predictions.append(pred)\n",
    "\n",
    "    avg_pred = np.mean(predictions, axis=0)\n",
    "    predicted_class = np.argmax(avg_pred[0])\n",
    "    confidence = np.max(avg_pred[0]) * 100\n",
    "\n",
    "    era_names = [era_name for _, _, era_name in ERA_RANGES]\n",
    "    predicted_era = era_names[predicted_class]\n",
    "\n",
    "    return predicted_era, confidence, avg_pred[0]\n",
    "\n",
    "\n",
    "ensemble_models = [resnet_model, efficientnet_model]\n",
    "print(\"\\nEnsemble model ready for predictions!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTesting ensemble prediction on sample images...\")\n",
    "sample_indices = np.random.choice(len(final_test_df), size=5, replace=False)\n",
    "for idx in sample_indices:\n",
    "    image_path = final_test_df.iloc[idx][\"filepath\"]\n",
    "    true_era_idx = int(final_test_df.iloc[idx][\"era_index\"])\n",
    "    true_era = ERA_RANGES[true_era_idx][2]\n",
    "\n",
    "    predicted_era, confidence, _ = ensemble_predict(ensemble_models, image_path)\n",
    "\n",
    "    print(f\"Image: {os.path.basename(image_path)}\")\n",
    "    print(f\"True era: {true_era}\")\n",
    "    print(f\"Predicted era: {predicted_era} (Confidence: {confidence:.2f}%)\")\n",
    "    print(\"---\")\n",
    "\n",
    "print(\"\\nTraining and evaluation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
